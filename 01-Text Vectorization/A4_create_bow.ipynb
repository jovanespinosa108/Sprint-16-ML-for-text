{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28103bc",
   "metadata": {},
   "source": [
    "### Creación de una bolsa de palabras\n",
    "\n",
    "En esta lección, vamos a aprender cómo crear una bolsa de palabras y encontrar palabras vacías.\n",
    "\n",
    "Para convertir un corpus de texto en una bolsa de palabras, usa la clase CountVectorizer() del módulo sklearn.feature_extraction.text.\n",
    "\n",
    "- Importa la clase\n",
    "- Crea un counter\n",
    "- Pasa el corpus de texto al contador. Llama a la función fit_transform(). El contador extrae palabras únicas del corpus y cuenta cuántas veces aparecen en cada texto del corpus. El contador no cuenta letras separadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88665e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a counter\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# bow = bolsa de palabras\n",
    "bow = count_vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b846b4e",
   "metadata": {},
   "source": [
    "Este método devuelve una matriz donde las filas representan textos y las columnas muestran palabras únicas del corpus. Los números en sus intersecciones representan cuántas veces aparece una determinada palabra en el texto.\n",
    "\n",
    "- Usemos el corpus (ya lematizado) de la lección anterior\n",
    "- Vamos a crear una bolsa de palabras para la matriz. Utiliza el atributo shape para descubrir el tamaño de la matriz\n",
    "- El resultado es 7 textos y 16 palabras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "# Create bow\n",
    "bow.shape(7,6)\n",
    "\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7d951",
   "metadata": {},
   "source": [
    "Aquí está nuestra bolsa de palabras como una matriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cadfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1]\n",
    " [0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1]\n",
    " [0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1]\n",
    " [0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1]\n",
    " [0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1]\n",
    " [0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1]\n",
    " [1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c692e1",
   "metadata": {},
   "source": [
    "La lista de palabras únicas en la bolsa se llama vocabulario; se almacena en el contador y se puede acceder a ella llamando al método get_feature_names():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feature_names()\n",
    "\n",
    "\"\"\"['all',\n",
    " 'and',\n",
    " 'battle',\n",
    " 'for',\n",
    " 'horse',\n",
    " 'horseshoe',\n",
    " 'kingdom',\n",
    " 'lost',\n",
    " 'message',\n",
    " 'nail',\n",
    " 'of',\n",
    " 'rider',\n",
    " 'shoe',\n",
    " 'the',\n",
    " 'want',\n",
    " 'was']\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafbef1",
   "metadata": {},
   "source": [
    "CountVectorizer() también se usa para cálculos de n-gramas. Especifica el tamaño del n-grama con el argumento ngram_range para que cuente las frases. Necesitarás dos números enteros para establecer el tamaño mínimo y máximo de n-gramas. Si solo necesitamos bigramas (frases de dos palabras), entonces lo haremos de esta manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45edc2",
   "metadata": {},
   "source": [
    "El contador funciona de la misma forma con frases y con palabras.\n",
    "\n",
    "Dado que un corpus grande representa una bolsa de palabras más grande, algunas de las palabras pueden mezclarse y terminar causando más confusión que claridad. Para ayudar con esto, por lo general, puedes eliminar las conjunciones y las preposiciones sin perder el significado de la oración. Si tienes una bolsa de palabras más pequeña y limpia, encontrarás más fácilmente las palabras más importantes para la clasificación del texto.\n",
    "\n",
    "Para asegurarte de obtener una bolsa de palabras más limpia, encuentra las palabras vacías (palabras que no significan nada por sí solas). Hay muchas de ellas, y son diferentes para cada idioma: en inglés, por ejemplo, hay artículos (\"a\", \"the\") y preposiciones (\"in\", \"for\").\n",
    "Echemos un vistazo al paquete stopwords del módulo nltk.corpus:\n",
    "\n",
    "Deberás descargar el paquete una vez para que funcione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Download stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00d8e5",
   "metadata": {},
   "source": [
    "Llama a la función stopwords.words() y utiliza 'english' como un argumento para obtener un conjunto de palabras vacías para inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7319d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d669836",
   "metadata": {},
   "source": [
    "Pasa la lista de palabras vacías al CountVectorizer() cuando crees el contador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d694bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7987b",
   "metadata": {},
   "source": [
    "Ahora el contador sabe qué palabras se deben excluir de la bolsa de palabras. Si lo ejecutas para el ejemplo anterior, la versión final de la bolsa de palabras se verá así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "['battle',\n",
    " 'horse',\n",
    " 'horseshoe',\n",
    " 'kingdom',\n",
    " 'lose',\n",
    " 'message',\n",
    " 'nail',\n",
    " 'rider',\n",
    " 'shoe',\n",
    " 'want']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684bbea",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "1. imdb_reviews_small_lemm.tsv contiene el conjunto de datos imdb_reviews_small.tsv al que agregamos la columna review_lemm con reseñas limpias y lematizadas.\n",
    "\n",
    "Crea dos bolsas de palabras para el corpus de reseñas: con y sin palabras vacías. Imprime sus tamaños (en precódigo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))# < escribe tu código aquí >\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# crea una bolsa de palabras con la comprobación de las palabras vacías\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(corpus) # < escribe tu código aquí >\n",
    "\n",
    "print('El tamaño de BoW con palabras vacías:', bow.shape)\n",
    "\n",
    "# crea una bolsa de palabras sin comprobar las palabras vacías\n",
    "count_vect = CountVectorizer(stop_words=stop_words)\n",
    "bow = count_vect.fit_transform(corpus) # < escribe tu código aquí >\n",
    "\n",
    "print('El tamaño de BoW sin palabras vacías:', bow.shape)\n",
    "\n",
    "# Output\n",
    "#El tamaño de BoW con palabras vacías: (4541, 26214)\n",
    "#El tamaño de BoW sin palabras vacías: (4541, 26098)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fec0c",
   "metadata": {},
   "source": [
    "Crea un contador de n-gramas para el corpus de reseñas. Cada frase debe tener dos palabras. Imprime el tamaño del n-grama (en precódigo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b733a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# importa CountVectorizer\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# crea un n-grama con n=2 y guárdalo en la variable n_gram\n",
    "count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "n_gram = count_vect.fit_transform(corpus)\n",
    "# < escribe tu código aquí >\n",
    "\n",
    "print('El tamaño del bigrama:', n_gram.shape)\n",
    "\n",
    "# Output\n",
    "#El tamaño del bigrama: (4541, 322321)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
