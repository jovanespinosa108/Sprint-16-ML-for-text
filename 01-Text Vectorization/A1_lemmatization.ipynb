{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb386cc",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "Antes de extraer características, vamos a tener que simplificar el texto. Esto ayudará a que el texto sea más fácil de manejar debido a las variaciones en las formas de las palabras.\n",
    "\n",
    "Estos son los pasos para el preprocesamiento de texto:\n",
    "\n",
    "1. Tokenización: dividir el texto en tokens (frases, palabras y símbolos separados);\n",
    "2. Lematización: reducir las palabras a sus formas fundamentales (lema).\n",
    "\n",
    "Puedes usar estas librerías tanto para la tokenización como para la lematización:\n",
    "\n",
    "- Natural Language Toolkit (NLTK)\n",
    "- spaCy\n",
    "\n",
    "Hay otras librerías que puedes usar para la tarea (por ejemplo, UDPipe, word2vec), pero NLTK y spaCy son las opciones más populares.\n",
    "\n",
    "Importa la función de tokenización y crea un objeto de lematización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e800f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f714f47",
   "metadata": {},
   "source": [
    "Pasa a la función lemmatize() el texto \"All models are wrong, but some are useful\" (\"Todos los modelos son incorrectos, pero algunos son útiles\") como tokens separados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162248a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jovan/nltk_data'\n    - 'c:\\\\Users\\\\jovan\\\\anaconda3\\\\envs\\\\ml_for_text_env\\\\nltk_data'\n    - 'c:\\\\Users\\\\jovan\\\\anaconda3\\\\envs\\\\ml_for_text_env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\jovan\\\\anaconda3\\\\envs\\\\ml_for_text_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jovan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mAll models are wrong, but some are useful.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m lemmas = [lemmatizer.lemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jovan\\anaconda3\\envs\\ml_for_text_env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jovan\\anaconda3\\envs\\ml_for_text_env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jovan\\anaconda3\\envs\\ml_for_text_env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jovan\\anaconda3\\envs\\ml_for_text_env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jovan\\anaconda3\\envs\\ml_for_text_env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jovan\\anaconda3\\envs\\ml_for_text_env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jovan/nltk_data'\n    - 'c:\\\\Users\\\\jovan\\\\anaconda3\\\\envs\\\\ml_for_text_env\\\\nltk_data'\n    - 'c:\\\\Users\\\\jovan\\\\anaconda3\\\\envs\\\\ml_for_text_env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\jovan\\\\anaconda3\\\\envs\\\\ml_for_text_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jovan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text = \"All models are wrong, but some are useful.\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f1fb92",
   "metadata": {},
   "source": [
    "La función word_tokenize() divide un texto en tokens y la función lemmatize() devuelve el lema de un token que se le pasó. Debido a que nos interesa lematizar una oración, el resultado generalmente se presenta como una lista de tokens lematizados.\n",
    "\n",
    "['all', 'model', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful', '.']\n",
    "\n",
    "También convertimos el texto a minúsculas porque así lo exige el lematizador NLTK.\n",
    "\n",
    "Usa la función join() para volver a convertir la lista de tokens procesados en una línea de texto, separando los elementos con un espacio opcional:\n",
    "\n",
    "\" \".join(lemmas)\n",
    "\n",
    "Obtenemos:\n",
    "\n",
    "'all model be wrong , but some be useful .'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37ab3c",
   "metadata": {},
   "source": [
    "spaCy (materiales en inglés) es otra librería popular para tokenizar y lematizar texto. El ejemplo anterior se puede procesar con spaCy de manera similar.\n",
    "La variable nlp significa \"procesamiento del lenguaje natural\" y contiene un modelo principal para el reconocimiento de texto. spaCy utiliza una pipeline especial para procesar el texto entrante. La línea nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) indica cargar el modelo de tamaño pequeño en inglés y deshabilitar algunos componentes de la pipeline. En el sitio web oficial hay más información sobre modelos (https://spacy.io/models) y pipelines (https://spacy.io/usage/processing-pipelines) (materiales en inglés).\n",
    "\n",
    "lemma_ es un atributo de un token que devuelve su forma base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07175d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "#Obtenemos:\n",
    "\n",
    "#all model be wrong , but some be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40c132",
   "metadata": {},
   "source": [
    "El resultado es un poco diferente, pero eso era de esperar. Las librerías de NLP a menudo comparten técnicas y principios, pero los implementan de diferentes maneras.\n",
    "\n",
    "En este capítulo usaremos reseñas de películas de IMDB.\n",
    "\n",
    "El conjunto de datos que usaremos en nuestros ejercicios contiene alrededor de 4,300 reseñas. (¡El conjunto de datos completo tiene 47,300!). A cada reseña se le ha asignado un \"código de tonalidad\", donde 0 significa reseñas negativas y 1, reseñas positivas.\n",
    "\n",
    "Importa los datos:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "\n",
    "Probablemente notaste que el archivo tiene una extensión \"tsv\", en lugar de la habitual \"csv\". TSV significa \"valores separados por tabuladores\". Es esencialmente el mismo formato que CSV, excepto por una diferencia. TSV usa el carácter de tabulación como delimitador en lugar del carácter de coma. Se eligió este formato para este conjunto de datos en particular para que fuera más fácil que aparecieran comas dentro de las reseñas.\n",
    "\n",
    "Un conjunto de textos se denomina colectivamente corpus. Por lo general, es necesario definir un corpus para una tarea de análisis de texto (por ejemplo, para construir un diccionario o un espacio vectorial basado en él). Cada registro de texto es tratado por un algoritmo de machine learning, de acuerdo con su \"posición\" en un corpus. No te preocupes, analizaremos todo esto con más detalle en las próximas lecciones.\n",
    "\n",
    "Crea un corpus a partir de las reseñas. Convierte la columna review en una lista de textos.\n",
    "\n",
    "corpus = data['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5cba7",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Escribe la función lemmatize(text) usando la librería spaCy. \n",
    "\n",
    "Esta debería convertir el texto a minúsculas y lematizarlo, devolviendo una cadena lematizada. \n",
    "\n",
    "Puedes tomar cualquier registro aleatorio del conjunto de datos. Si te parece que son demasiado largos (después de todo, ¡son reseñas!), utiliza el registro de texto No. 2557 (este es una reseña breve) de imdb_reviews_small.tsv. \n",
    "\n",
    "Imprime tanto el texto inicial como el lematizado (en precódigo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random             # para seleccionar una reseña aleatoria\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmas)# < escribe tu código aquí >\n",
    "\n",
    "# guarda el índice de revisión en la variable review_idx\n",
    "# ya sea como un número aleatorio o un valor fijo, por ejemplo, 2557\n",
    "#review_idx = random.randint(0, len(corpus)-1)\n",
    "review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print('El texto original:', review)\n",
    "print()\n",
    "print('El texto lematizado:', lemmatize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45577eb",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "El texto original: I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
    "\n",
    "El texto lematizado: I like this show from the first episode I see , which be the \" rhapsody in blue \" episode ( for those that do not know what that be , the zan go insane and become pau lvl 10 ep ) . good visual and special effect I 've see on a television series , nothing like it anywhere ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe0892",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
