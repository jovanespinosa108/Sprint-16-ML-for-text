{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1270b778",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "Vamos a echar un vistazo al modelo BERT para descubrir c√≥mo ayuda a convertir palabras en vectores.\n",
    "\n",
    "BERT (Representaciones de Codificador Bidireccional de Transformadores) es un modelo de red neuronal creado para la representaci√≥n del lenguaje. Fue creado por Google para mejorar la relevancia de los resultados de b√∫squeda y se public√≥ en 2018 (el art√≠culo original se encuentra en:https://arxiv.org/abs/1810.04805 (Estos materiales est√°n en ingl√©s)). Este algoritmo es capaz de \"comprender\" el contexto de un texto completo, no solo de frases cortas. BERT se usa con frecuencia en el machine learning para convertir textos en vectores. Los especialistas suelen utilizar modelos BERT existentes que est√°n previamente entrenados (por Google o, posiblemente, por otros colaboradores) en grandes corpus de texto. Los modelos BERT previamente entrenados funcionan para muchos idiomas (104, con exactitud). Puedes entrenar tu propio modelo de representaci√≥n del lenguaje, pero este requerir√° muchos recursos computacionales. \n",
    "\n",
    "BERT es un paso evolutivo en comparaci√≥n con word2vec. BERT se convirti√≥ r√°pidamente en la opci√≥n popular para los programadores y ha inspirado a los investigadores a crear otros modelos de representaci√≥n de lenguaje: FastText, GloVe (Vectores globales para representaci√≥n de palabras), ELMO (Insertados del modelo de lenguaje), GPT (Transformador generativo de preentrenamiento). Los modelos m√°s precisos actualmente son BERT y GPT.\n",
    "\n",
    "Al procesar palabras, BERT considera tanto las palabras vecinas inmediatas como las palabras m√°s lejanas. Esto permite que BERT produzca vectores precisos con respecto al significado natural de las palabras.\n",
    "\n",
    "As√≠ es como funciona:\n",
    "\n",
    "Aqu√≠ hay un ejemplo de entrada para el modelo: \"The red beak of the puffin [MASK] in the blue [MASK] \", donde MASK representa palabras desconocidas o enmascaradas. El modelo tiene que adivinar cu√°les son estas palabras enmascaradas.\n",
    "El modelo aprende a averiguar si las palabras del enunciado est√°n relacionadas. Ten√≠amos enmascaradas las palabras \"flashed\" y \"sky\". El modelo tiene que comprender que una palabra sigue a la otra. Entonces, si ocult√°ramos la palabra \"crawled\" en lugar de \"flashed\", el modelo no encontrar√≠a una conexi√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998492d2",
   "metadata": {},
   "source": [
    "üìò BERT (Bidirectional Encoder Representations from Transformers)\n",
    "üîπ ¬øQu√© es BERT?\n",
    "\n",
    "BERT es un modelo de red neuronal para la representaci√≥n del lenguaje.\n",
    "\n",
    "Fue creado por Google en 2018 para mejorar la relevancia de los resultados de b√∫squeda.\n",
    "\n",
    "El art√≠culo original se encuentra aqu√≠: arxiv.org/abs/1810.04805\n",
    ".\n",
    "\n",
    "A diferencia de modelos anteriores, BERT comprende el contexto completo de un texto, no solo frases aisladas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd1976",
   "metadata": {},
   "source": [
    "üîπ ¬øPara qu√© se usa?\n",
    "\n",
    "BERT convierte textos en vectores num√©ricos (embeddings).\n",
    "\n",
    "Se utiliza en machine learning y NLP para tareas como:\n",
    "\n",
    "An√°lisis de sentimiento\n",
    "\n",
    "Clasificaci√≥n de texto\n",
    "\n",
    "B√∫squedas sem√°nticas\n",
    "\n",
    "Respuestas autom√°ticas\n",
    "\n",
    "Generalmente, los especialistas emplean modelos BERT preentrenados (por Google u otros equipos).\n",
    "\n",
    "Estos modelos est√°n entrenados en grandes corpus y funcionan en 104 idiomas.\n",
    "\n",
    "Es posible entrenar un BERT propio, pero requiere much√≠simos recursos computacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb7f60",
   "metadata": {},
   "source": [
    "üîπ BERT vs otros modelos\n",
    "\n",
    "Evoluci√≥n respecto a Word2Vec.\n",
    "\n",
    "Inspir√≥ la creaci√≥n de otros modelos de representaci√≥n de lenguaje:\n",
    "\n",
    "FastText\n",
    "\n",
    "GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "ELMo (Embeddings from Language Models)\n",
    "\n",
    "GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "üëâ Actualmente, BERT y GPT son los m√°s precisos y utilizados.\n",
    "\n",
    "üîπ ¬øC√≥mo funciona BERT?\n",
    "\n",
    "BERT analiza tanto las palabras vecinas inmediatas como las lejanas.\n",
    "\n",
    "Esto le permite generar vectores precisos que reflejan el significado real de las palabras en contexto.\n",
    "\n",
    "Ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33337f",
   "metadata": {},
   "source": [
    "Texto de entrada:\n",
    "\n",
    "\"The red beak of the puffin [MASK] in the blue [MASK]\"\n",
    "\n",
    "\n",
    "[MASK] indica una palabra desconocida que el modelo debe predecir.\n",
    "\n",
    "En este caso, las palabras ocultas eran \"flashed\" y \"sky\".\n",
    "\n",
    "El modelo debe entender la relaci√≥n: ‚Äúflashed‚Äù ocurre antes de ‚Äúsky‚Äù.\n",
    "\n",
    "Si reemplaz√°ramos por \"crawled\", BERT detectar√≠a que no hay conexi√≥n sem√°ntica.\n",
    "\n",
    "üìå Resumen corto:\n",
    "BERT es un modelo de Google que convierte texto en vectores teniendo en cuenta el contexto completo. Es muy usado en NLP y marc√≥ una evoluci√≥n frente a t√©cnicas anteriores como Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512e6b0",
   "metadata": {},
   "source": [
    "BERT y preprocesamiento\n",
    "Vamos a resolver una tarea de clasificaci√≥n para rese√±as de pel√≠culas usando la representaci√≥n del lenguaje BERT, es decir, usando BERT para crear vectores para palabras. Vamos a tomar un modelo previamente entrenado llamado bert-base-uncased (entrenado en textos en ingl√©s en min√∫sculas).\n",
    "\n",
    "Para esta lecci√≥n, usaremos un c√≥digo prefabricado que no necesita cambios. Esta pr√°ctica no tiene nada de malo. Es com√∫n que los programadores copien y usen fragmentos de c√≥digo existentes. Tu tarea ser√° hacer que funcione.\n",
    "\n",
    "Entonces, ¬øcu√°l es la tarea? Tenemos un gran conjunto de datos de rese√±as de pel√≠culas y necesitamos entrenar la m√°quina para diferenciar entre rese√±as positivas y negativas.\n",
    "\n",
    "Vamos a resolver esta tarea usando las librer√≠as PyTorch y transformers. La primera librer√≠a se utiliza para trabajar con modelos de redes neuronales, mientras que la segunda implementa BERT y otros modelos de representaci√≥n del lenguaje. Vamos a importarlas:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01dc06",
   "metadata": {},
   "source": [
    "Como instalar librerias\n",
    "\n",
    "1. Verifica tu versi√≥n de Python\n",
    "\n",
    "PyTorch conda requiere Python 3.8 ‚Äì 3.11 (a 2025).\n",
    "En tu terminal:\n",
    "\n",
    "python --version\n",
    "\n",
    "\n",
    "Si est√°s en 3.12, mejor crea un ambiente nuevo con 3.10 (recomendado):\n",
    "\n",
    "conda create -n torch_env python=3.10 -y\n",
    "conda activate torch_env\n",
    "\n",
    "2. Instala PyTorch limpio con CPU (para evitar problemas con CUDA al inicio)\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "\n",
    "\n",
    "Esto funciona en cualquier PC, incluso sin GPU.\n",
    "\n",
    "3. Si quieres GPU (CUDA)\n",
    "\n",
    "Primero revisa si tu tarjeta NVIDIA tiene drivers y CUDA instalados:\n",
    "\n",
    "nvidia-smi\n",
    "\n",
    "\n",
    "(Si esto falla, no uses CUDA a√∫n).\n",
    "\n",
    "Si funciona, instala PyTorch con soporte CUDA:\n",
    "\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "4. Reinstala desde cero si el error persiste\n",
    "\n",
    "A veces hay conflicto pip-conda. Haz:\n",
    "\n",
    "conda remove pytorch torchvision torchaudio --force\n",
    "pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "\n",
    "Y reinstala solo con conda como en los pasos 2 o 3.\n",
    "\n",
    "5. Prueba la instalaci√≥n\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef31e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jovan\\anaconda3\\envs\\torch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03693591",
   "metadata": {},
   "source": [
    "Antes de convertir textos en vectores, necesitamos preprocesar el texto. BERT tiene su propio tokenizador basado en el corpus en el que fue entrenado. Otros tokenizadores no funcionan con BERT y no requieren lematizaci√≥n.\n",
    "\n",
    "Pasos de preprocesamiento para el texto:\n",
    "\n",
    "Inicializa el tokenizador como una instancia de BertTokenizer() con el nombre del modelo previamente entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d2c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9686, 14163, 2100, 10975, 28804, 2080, 21183, 18622, 9057, 10938, 26467, 2229, 102]\n"
     ]
    }
   ],
   "source": [
    "# Inicializa el tokenizador como una instancia de BertTokenizer() \n",
    "# con el nombre del modelo previamente entrenado.\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convierte el texto en ID de tokens y el tokenizador BERT \n",
    "# devolver√° los ID de tokens en lugar de los tokens:\n",
    "example = 'Es muy pr√°ctico utilizar transformadores'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadf5e8",
   "metadata": {},
   "source": [
    "Considera que los identificadores de token anteriores son esencialmente √≠ndices num√©ricos para tokens en el diccionario interno utilizado por BERT. Tambi√©n debes saber que el diccionario se us√≥ para entrenar BERT previamente, es parte del modelo BERT y est√° cargado con el m√©todo from_pretrained.\n",
    "\n",
    "Para operar el modelo correctamente, establecemos el argumento add_special_tokens en True. Significa que agregamos el token inicial (101) y el token final (102) a cualquier texto que se est√© transformando.\n",
    "\n",
    "BERT acepta vectores de una longitud fija, por ejemplo, de 512 tokens. Si no hay suficientes palabras en una cadena de entrada para completar todo el vector con tokens (o, m√°s bien, sus identificadores), el final del vector se rellena con ceros. Si hay demasiadas palabras y la longitud del vector excede 510 (recuerda que se reservan dos posiciones para los tokens de inicio y finalizaci√≥n), o bien la cadena de entrada se limita al tama√±o de 510, o bien se suelen omitir algunos identificadores devueltos por tokenizer.encode(), por ejemplo, todos los identificadores despu√©s de la posici√≥n 512 en la lista:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc253b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  9686 14163  2100 10975 28804  2080 21183 18622  9057 10938 26467\n",
      "  2229   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "n = 512\n",
    "\n",
    "padded = np.array(ids[:n] + [0]*(n - len(ids)))\n",
    "\n",
    "print(padded)\n",
    "\n",
    "# Obtenemos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dbec4",
   "metadata": {},
   "source": [
    "Ahora tenemos que decirle al modelo por qu√© los ceros no tienen informaci√≥n significativa. Esto es necesario para el componente del modelo que se llama attention. Vamos a descartar estos tokens y crear una m√°scara para los tokens importantes, indicando valores cero y distintos de cero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602e4384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4d394",
   "metadata": {},
   "source": [
    "Podemos establecer manualmente la longitud m√°xima de la entrada. Para ello, establece el par√°metro max_length en el valor deseado y especifica truncation=True. Esto cortar√° los tokens que excedan el l√≠mite. Ten en cuenta que la parte resultante del texto inicial puede ser incluso m√°s peque√±a si add_special_token es True.\n",
    "Considera este ejemplo intacto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18fc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2009, 2003, 2200, 18801, 2000, 2224, 19081, 102]\n"
     ]
    }
   ],
   "source": [
    "example = 'It is very handy to use transformers'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcdcd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2009, 2003, 2200, 102]\n"
     ]
    }
   ],
   "source": [
    "# El truncado\n",
    "example = 'It is very handy to use transformers'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True, \n",
    "               max_length=5, truncation=True)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a484c8",
   "metadata": {},
   "source": [
    "5 tokens, tal como se esperaba, pero solo hay tres palabras presentes: max_length - tokens especiales = 5 - 2 = 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
